{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06021f-5cff-43f5-aee2-0d4d4b5b49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Set color map to have light blue background\n",
    "sns.set()\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ff416-5bff-4bd9-b3a3-fb2bf11f27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('BeijingPM20100101_20151231.csv')\n",
    "\n",
    "print('INFO:')\n",
    "df.info()\n",
    "\n",
    "print(\"====================\")\n",
    "print('HEAD:')\n",
    "print(df.head())\n",
    "\n",
    "print(\"====================\")\n",
    "# if a column is numeric, show the number of nan values in it\n",
    "for col_name in df.columns:\n",
    "    if np.issubdtype(df[col_name].dtype, np.number): \n",
    "        print(\"Column {} is numeric, #nan is {}\".format(col_name, np.count_nonzero(np.isnan(df[col_name].values))))\n",
    "    else:\n",
    "        print(\"Column {} is not numeric\".format(col_name))\n",
    "\n",
    "print(\"====================\")\n",
    "print('DESCRIBE:')\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac43834-4760-4968-9bbb-87033604c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns of no use here\n",
    "df = df.drop(columns=['No', 'season', 'precipitation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481918a-be84-477c-8fc5-8b01876eca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four PM columns\n",
    "df_pms = df[['PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'PM_US Post']]\n",
    "# drop nan values from the four columns\n",
    "df_pms = df_pms.dropna()\n",
    "# plot the correlation matrix of the four pm columns\n",
    "ax_pms = sns.heatmap(df_pms.corr(), vmin=-1.0, vmax=1.0, annot=True, fmt='.2g', center=0.0, xticklabels=True, yticklabels=True)\n",
    "# ax_pms.figure.set_size_inches(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fcd72-905c-439b-b499-68dcad9ba6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot of the four pm columns\n",
    "# suppress warnings\n",
    "with warnings.catch_warnings(record=True):\n",
    "    sns.pairplot(df_pms, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2416cb9-0c01-4643-8254-e5d0abad8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the four columns as the final PM 2.5 level\n",
    "df_pm = df[['PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'PM_US Post']]\n",
    "print(\"====================\")\n",
    "df_pm.info()\n",
    "# mean of the four PM columns with valid values\n",
    "df_pm_mean = df_pm.mean(axis=1)\n",
    "print(\"====================\")\n",
    "df_pm_mean.info()\n",
    "df['PM'] = df_pm_mean\n",
    "# drop the four original PM 2.5 columns\n",
    "df = df.drop(columns=['PM_Dongsi', 'PM_Dongsihuan', 'PM_Nongzhanguan', 'PM_US Post'])\n",
    "# add final PM 2.5 column\n",
    "df[\"PM\"] = df_pm_mean\n",
    "print(\"====================\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbd9d6-60ca-434c-99eb-8db0c5ac4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N: 0, NE: 45, E: 90, SE: 135, S: 180, SW: 225, W: 270, NW: 315\n",
    "direction_dict = {\n",
    "    'N': 0,\n",
    "    'NE': 45,\n",
    "    'E': 90,\n",
    "    'SE': 135,\n",
    "    'S': 180,\n",
    "    'SW': 225,\n",
    "    'W': 270,\n",
    "    'NW': 315\n",
    "}\n",
    "sin_dict = {k: np.sin(direction_dict[k] / 180 * np.pi) for k in direction_dict.keys()}\n",
    "cos_dict = {k: np.cos(direction_dict[k] / 180 * np.pi) for k in direction_dict.keys()}\n",
    "print(sin_dict)\n",
    "print(cos_dict)\n",
    "\n",
    "df_cbwd = df['cbwd']\n",
    "\n",
    "# some rows get 'cv' in this column, I find it hard to assign the sine and cosine terms for these rows\n",
    "# so the 'cv' rows are assigned 0 in the sine column and the cosine column\n",
    "def sin_func(x):\n",
    "    if x == 'cv':\n",
    "        return 0\n",
    "    if x == 'NA' or x != x:\n",
    "        return np.nan\n",
    "    return sin_dict[x]\n",
    "\n",
    "def cos_func(x):\n",
    "    if x == 'cv':\n",
    "        return 0\n",
    "    if x == 'NA' or x != x:\n",
    "        return np.nan\n",
    "    return cos_dict[x]\n",
    "\n",
    "def is_cv(x):\n",
    "    if x == 'cv':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# add sine and cosine columns\n",
    "df['sin_cbwd'] = df_cbwd.apply(sin_func)\n",
    "df['cos_cbwd'] = df_cbwd.apply(cos_func)\n",
    "# add is_cv column\n",
    "df['is_cv'] = df_cbwd.apply(is_cv)\n",
    "# drop the original 'cwbd' column\n",
    "df = df.drop(columns=['cbwd'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d3a3c-8eda-4f20-af42-9400fa3d1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop rows with nan values\n",
    "df = df.dropna()\n",
    "print(\"INFO:\")\n",
    "df.info()\n",
    "print(\"====================\")\n",
    "print(\"DESCRIBE:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b9c632-83ab-4ab6-9fb5-cada827ce256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data histograms\n",
    "df.hist(bins=30, figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Drop cases with extremely high PM 2.5 levels\n",
    "df = df[df['PM'] <= 500]\n",
    "\n",
    "print(\"INFO:\")\n",
    "df.info()\n",
    "print(\"====================\")\n",
    "print(\"DESCRIBE:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be0ffe-343f-45cb-97e8-b72f4bbcc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.mean(axis=0)\n",
    "df_std = df.std(axis=0)\n",
    "print('mean:')\n",
    "print(df_mean)\n",
    "print(\"====================\")\n",
    "print('std:')\n",
    "print(df_std)\n",
    "# standardization\n",
    "for col_name in df.columns:\n",
    "    if col_name != 'PM' and col_name != 'is_cv' and col_name != 'sin_cbwd' and col_name != 'cos_cbwd':\n",
    "        df[col_name] = df[col_name].apply(lambda x: (x - df_mean[col_name]) / df_std[col_name])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57880f-567e-4276-a8bb-1a5421c2cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)\n",
    "ax = sns.heatmap(df.corr(), vmin=-1.0, vmax=1.0, annot=True, fmt='.2g', center=0.0, xticklabels=True, yticklabels=True)\n",
    "ax.figure.set_size_inches(16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2ad79-d02f-47e4-a44d-ac59b29a59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "# suppress warnings\n",
    "with warnings.catch_warnings(record=True):\n",
    "    sns.pairplot(df, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b5ec0-db87-4917-b38e-62c086cf83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression with all features\n",
    "formula_str = 'PM ~ '\n",
    "num_feats_added = 0\n",
    "for i in range(len(df.columns)):\n",
    "    col_name = df.columns[i]\n",
    "    if col_name != 'PM':\n",
    "        if num_feats_added != 0:\n",
    "            formula_str += ' + '\n",
    "        formula_str += col_name\n",
    "        num_feats_added += 1\n",
    "\n",
    "print(formula_str)\n",
    "model_simple = smf.ols(formula=formula_str, data=df).fit()\n",
    "print(model_simple.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00738e6-e71f-478a-ad36-ef097699d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=13)\n",
    "print(\"train size: {}, test size: {}\".format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23000edc-fe4a-4636-87eb-b5cf4bd23ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed selection with p-value threshold 0.025\n",
    "p_thresh = 0.025\n",
    "# collect feature names\n",
    "feature_names = []\n",
    "for col_name in train_dataset.columns:\n",
    "    if col_name != 'PM':\n",
    "        feature_names.append(col_name)\n",
    "\n",
    "formula_str = 'PM ~ '\n",
    "selected_features = []\n",
    "\n",
    "num_feat = len(feature_names)\n",
    "for i in range(num_feat):\n",
    "    # in each iteration, save the correspinding score of each feature\n",
    "    r_squared_adj_list = []\n",
    "    for feat in feature_names:\n",
    "        formula_str_tmp = concat_formula(formula_str, feat, i)\n",
    "        model_tmp = smf.ols(formula=formula_str_tmp, data=train_dataset).fit()\n",
    "        r_squared_adj_list.append((feat, model_tmp.rsquared_adj))\n",
    "    # sort the features by adjusted R-squared\n",
    "    r_squared_adj_list = sorted(r_squared_adj_list, key=lambda x: x[1], reverse=True)\n",
    "    print(r_squared_adj_list)\n",
    "    # the best feature is the one gives the best adjusted R-squared\n",
    "    # select the best feature and add it into the model\n",
    "    best_feat = r_squared_adj_list[0][0]\n",
    "    feature_names.remove(best_feat)\n",
    "    selected_features.append(best_feat)\n",
    "    formula_str = concat_formula(formula_str, best_feat, i)\n",
    "    \n",
    "    # check if there are insignificant features in the new model (p-value > threshold)\n",
    "    while True:\n",
    "        print(\"new formula: {}\".format(formula_str))\n",
    "        model_best = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "        # print(model_best.summary())\n",
    "        pvals = []\n",
    "        for feat in selected_features:\n",
    "            pvals.append((feat, model_best.pvalues[feat]))\n",
    "        pvals = sorted(pvals, key=lambda x: x[1], reverse=True)\n",
    "        if pvals[0][1] > p_thresh:\n",
    "            # if the feature with the worst p-value is too bad, it is removed\n",
    "            selected_features.remove(pvals[0][0])\n",
    "            formula_str = concat_formula_from_empty(selected_features)\n",
    "            continue\n",
    "        else:\n",
    "            # if the p values of all the features are small enough\n",
    "            break\n",
    "\n",
    "# the model obtained from mixed selected\n",
    "model_selected = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "print(model_selected.summary())\n",
    "\n",
    "# for the selected features, draw a plot of adjusted R squared scores vs. features added and mse vs. features added\n",
    "formula_str = 'PM ~ '\n",
    "r_squared_list_train = []\n",
    "mse_list_train = []\n",
    "r_squared_list_test = []\n",
    "mse_list_test = []\n",
    "for i in range(len(selected_features)):\n",
    "    formula_str = concat_formula(formula_str, selected_features[i], i)\n",
    "    model_tmp_train = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "    r_squared_list_train.append(model_tmp_train.rsquared_adj)\n",
    "    _, _, _, mse_train = calculate_rsquared_and_mse(model_tmp_train, train_dataset)\n",
    "    mse_list_train.append(mse_train)\n",
    "    _, rsquared_adj_test, _, mse_test = calculate_rsquared_and_mse(model_tmp_train, test_dataset)\n",
    "    r_squared_list_test.append(rsquared_adj_test)\n",
    "    mse_list_test.append(mse_test)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax0.set_title(\"Adjusted R-squared vs. Added Features\")\n",
    "ax0.plot(selected_features, r_squared_list_train, label=\"train\")\n",
    "ax0.plot(selected_features, r_squared_list_test, label='test')\n",
    "for tick in ax0.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "ax1.set_title(\"MSE vs. Added Features\")\n",
    "ax1.plot(selected_features, mse_list_train, label='train')\n",
    "ax1.plot(selected_features, mse_list_test, label='test')\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f26b5-8bef-4876-b9b6-62704d88580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features dropped after mixed selection\n",
    "features_dropped = ['day', 'year', 'month', 'sin_cbwd', 'TEMP']\n",
    "# features left after mixed selection\n",
    "features = ['HUMI', 'cos_cbwd', 'DEWP', 'Iws', 'hour', 'PRES', 'Iprec', 'is_cv']\n",
    "\n",
    "formula_str = concat_formula_from_empty(features)\n",
    "model = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4681d-5f1b-41d3-a4dd-be4b06a95c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use VIF threshold = 10, vif scores greater than this threshold means that high correlation exists\n",
    "vif_thresh = 10\n",
    "\n",
    "while True:\n",
    "    vif_dataset = train_dataset[features]\n",
    "    vif_scores = [VIF(vif_dataset, i) for i in range(len(features))]\n",
    "    print(vif_scores)\n",
    "    idx_max = np.argmax(vif_scores)\n",
    "    if vif_scores[idx_max] > vif_thresh:\n",
    "        # the worst feature with vif > threshold should be removed \n",
    "        print(\"{} vif: {}, removed\".format(features[idx_max], vif_scores[idx_max]))\n",
    "        features_dropped.append(features[idx_max])\n",
    "        features.remove(features[idx_max])\n",
    "    else:\n",
    "        # all the remaining features are good\n",
    "        break\n",
    "\n",
    "print(\"features: {}\".format(features))\n",
    "formula_str = concat_formula_from_empty(features)\n",
    "model = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# save the best adjusted R-squared score for later use\n",
    "rsquared_adj_best = model.rsquared_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60e0e9-b3d8-4c8d-88e9-f575602e95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to order 10\n",
    "n = 10\n",
    "orders = list(range(1, n + 1))\n",
    "\n",
    "for feat in features:\n",
    "    # for each feature, draw Adjusted R-suqared vs. orders and MSE vs. orders curves\n",
    "    # then select the best order according to the plot manually\n",
    "    formula_str = concat_formula_from_empty(features)\n",
    "    r_squared_list_train = []\n",
    "    mse_list_train = []\n",
    "    r_squared_list_test = []\n",
    "    mse_list_test = []\n",
    "    \n",
    "    for k in orders:\n",
    "        if k == 1:\n",
    "            pass\n",
    "        else:\n",
    "            formula_str = formula_str + ' + np.power({}, {})'.format(feat, k)\n",
    "        # print(formula_str)\n",
    "        model_tmp_train = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "        r_squared_list_train.append(model_tmp_train.rsquared_adj)\n",
    "        _, _, _, mse_train = calculate_rsquared_and_mse(model_tmp_train, train_dataset)\n",
    "        mse_list_train.append(mse_train)\n",
    "        _, r_squared_adj_test, _, mse_test = calculate_rsquared_and_mse(model_tmp_train, test_dataset)\n",
    "        r_squared_list_test.append(r_squared_adj_test)\n",
    "        mse_list_test.append(mse_test)\n",
    "\n",
    "    # draw Adjusted R-suqared vs. orders and MSE vs. orders curves\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.set_title(\"Adjusted R-squared vs. Added Features of High Orders ({})\".format(feat))\n",
    "    ax0.plot(orders, r_squared_list_train, label=\"train\")\n",
    "    ax0.plot(orders, r_squared_list_test, label='test')\n",
    "    for tick in ax0.get_xticklabels():\n",
    "        tick.set_rotation(75)\n",
    "    \n",
    "    ax1.set_title(\"MSE vs. Added Features of High Orders ({})\".format(feat))\n",
    "    ax1.plot(orders, mse_list_train, label='train')\n",
    "    ax1.plot(orders, mse_list_test, label='test')\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(75)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6da1ab-a4f6-4a71-a9e8-8ceaf0bf7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_features\n",
    "features_all = features[:]\n",
    "# high-order terms selected according to the curves output from the previous cell\n",
    "features_all.append(\"np.power({}, {})\".format('DEWP', 2))\n",
    "features_all.append(\"np.power({}, {})\".format('DEWP', 3))\n",
    "features_all.append(\"np.power({}, {})\".format('DEWP', 4))\n",
    "features_all.append(\"np.power({}, {})\".format('PRES', 2))\n",
    "features_all.append(\"np.power({}, {})\".format('hour', 2))\n",
    "features_all.append(\"np.power({}, {})\".format('hour', 3))\n",
    "features_all.append(\"np.power({}, {})\".format('hour', 4))\n",
    "\n",
    "# the model with high-order terms added\n",
    "formula_str = concat_formula_from_empty(features_all)\n",
    "\n",
    "model = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3c56a-80e8-4dc3-b8ae-d74bbb225c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction features\n",
    "features_interactions = []\n",
    "for i in range(len(features)):\n",
    "    for j in range(i + 1, len(features)):\n",
    "        features_interactions.append(\"{}:{}\".format(features[i], features[j]))\n",
    "\n",
    "num_feat_inter = len(features_interactions)\n",
    "formula_str = concat_formula_from_empty(features_all)\n",
    "# using mixed selection to select interaction terms\n",
    "for i in range(num_feat_inter):\n",
    "    rsquared_adj_list = []\n",
    "    for feat_inter in features_interactions:\n",
    "        formula_str_tmp = concat_formula_from_another(formula_str, [feat_inter])\n",
    "        # print(formula_str_tmp)\n",
    "        model_tmp = smf.ols(formula=formula_str_tmp, data=train_dataset).fit()\n",
    "        rsquared_adj_list.append((feat_inter, model_tmp.rsquared_adj))\n",
    "    \n",
    "    # select the interaction term with best adjusted R-squared value from remaining interaction terms\n",
    "    rsquared_adj_list = sorted(rsquared_adj_list, key=lambda x: x[1], reverse=True)\n",
    "    best_feat_inter = rsquared_adj_list[0][0]\n",
    "\n",
    "    # check if insignificant features exist after a new interaction term is added\n",
    "    formula_str = concat_formula_from_another(formula_str, [best_feat_inter])\n",
    "    features_all.append(best_feat_inter)\n",
    "    while True:\n",
    "        model_tmp = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "    \n",
    "        pvals = []\n",
    "        for feat in features_all:\n",
    "            pvals.append((feat, model_tmp.pvalues[feat]))\n",
    "        pvals = sorted(pvals, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # find the interaction term with the worst p-value\n",
    "        worst_feat_inter = None\n",
    "        worst_feat_inter_pval = 1\n",
    "        for k in range(len(pvals)):\n",
    "            if pvals[k][0] in features_interactions:\n",
    "                worst_feat_inter = pvals[k][0]\n",
    "                worst_feat_inter_pval = pvals[k][1]\n",
    "                break\n",
    "        if worst_feat_inter_pval > p_thresh:\n",
    "            features_all.remove(worst_feat_inter)\n",
    "            formula_str = concat_formula_from_empty(features_all)\n",
    "        else:\n",
    "            # all p-values of interaction terms are small enough\n",
    "            break\n",
    "\n",
    "\n",
    "formula_str = concat_formula_from_empty(features_all)\n",
    "model = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# check the adjusted R-squared curves and mse curves\n",
    "r_squared_list_train = []\n",
    "mse_list_train = []\n",
    "r_squared_list_test = []\n",
    "mse_list_test = []\n",
    "\n",
    "formula_str = 'PM ~ '\n",
    "for i in range(len(features_all)):\n",
    "    feat = features_all[i]\n",
    "    formula_str = concat_formula(formula_str, feat, i)\n",
    "    model_tmp = smf.ols(formula=formula_str, data=train_dataset).fit()\n",
    "    r_squared_list_train.append(model_tmp.rsquared_adj)\n",
    "    _, _, _, mse_train = calculate_rsquared_and_mse(model_tmp, train_dataset)\n",
    "    mse_list_train.append(mse_train)\n",
    "    _, rsquared_adj_test, _, mse_test = calculate_rsquared_and_mse(model_tmp, test_dataset)\n",
    "    r_squared_list_test.append(rsquared_adj_test)\n",
    "    mse_list_test.append(mse_test)\n",
    "\n",
    "# for the selected features, draw a plot of adjusted R squared scores vs. features added and MSE vs. features added\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "\n",
    "ax0.set_title(\"Adjusted R-squared vs. Added Features\")\n",
    "ax0.plot(features_all, r_squared_list_train, label=\"train\")\n",
    "ax0.plot(features_all, r_squared_list_test, label='test')\n",
    "for tick in ax0.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "ax1.set_title(\"MSE vs. Added Features\")\n",
    "ax1.plot(features_all, mse_list_train, label='train')\n",
    "ax1.plot(features_all, mse_list_test, label='test')\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c22e79-a32f-42b6-8e33-c65b8d43d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first two interaction features in features_all according to the curves output from the previous cell\n",
    "features_all = features_all[:17]\n",
    "formula_str = concat_formula_from_empty(features_all)\n",
    "model = smf.ols(formula_str, data=train_dataset).fit()\n",
    "print(model.summary())\n",
    "\n",
    "reg_rsquared_test, reg_rsquared_adj_test, reg_mse_resid_test, reg_mse_test = calculate_rsquared_and_mse(model, test_dataset)\n",
    "_, _, _, reg_mse_train = calculate_rsquared_and_mse(model, train_dataset)\n",
    "\n",
    "reg_rsquared_train = model.rsquared\n",
    "reg_rsquared_adj_train = model.rsquared_adj\n",
    "reg_mse_resid_train = model.mse_resid\n",
    "\n",
    "print(\"training R-squared: {}\".format(reg_rsquared_train))\n",
    "print(\"testing R-squared: {}\".format(reg_rsquared_test))\n",
    "print(\"training adjusted R-squared: {}\".format(reg_rsquared_adj_train))\n",
    "print(\"testing adjusted R-squared: {}\".format(reg_rsquared_adj_test))\n",
    "print(\"training mse of residuals: {}\".format(reg_mse_resid_train))\n",
    "print(\"testing mse of residuals: {}\".format(reg_mse_resid_test))\n",
    "print(\"training mse: {}\".format(reg_mse_train))\n",
    "print(\"testing mse: {}\".format(reg_mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3774072-2964-4306-a1ad-4ed442b756db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected in 2.2\n",
    "features8 = ['HUMI', 'cos_cbwd', 'DEWP', 'Iws', 'hour', 'PRES', 'Iprec', 'is_cv']\n",
    "\n",
    "train_X = train_dataset[features8].values\n",
    "train_y = train_dataset['PM'].values\n",
    "test_X = test_dataset[features8].values\n",
    "test_y = test_dataset['PM'].values\n",
    "print(\"train_X shape: {}\".format(train_X.shape))\n",
    "print(\"train_y shape: {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eba6ef-6df1-435c-b1fe-b03f4dbc2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large learning rate will break the boosting iterations, use a small learning rate here\n",
    "lr_hint = 0.01\n",
    "\n",
    "# select the best loss\n",
    "losses = ['linear', 'square', 'exponential']\n",
    "rsquared_list_train = []\n",
    "mse_list_train = []\n",
    "rsquared_list_test = []\n",
    "mse_list_test = []\n",
    "for l in losses:\n",
    "    regressor = AdaBoostRegressor(random_state=13, learning_rate=lr_hint, loss=l)\n",
    "    regressor.fit(train_X, train_y)\n",
    "    rsquared_train, mse_train = calculate_rsquared_and_mse_adaboost(regressor, train_X, train_y)\n",
    "    rsquared_test, mse_test = calculate_rsquared_and_mse_adaboost(regressor, test_X, test_y)\n",
    "    rsquared_list_train.append(rsquared_train)\n",
    "    mse_list_train.append(mse_train)\n",
    "    rsquared_list_test.append(rsquared_test)\n",
    "    mse_list_test.append(mse_test)\n",
    "    \n",
    "# draw a plot of R squared scores vs. losses and MSE vs. losses\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "\n",
    "ax0.set_title(\"R-squared vs. Losses\")\n",
    "ax0.plot(losses, rsquared_list_train, label=\"train\")\n",
    "ax0.plot(losses, rsquared_list_test, label='test')\n",
    "for tick in ax0.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "ax1.set_title(\"MSE vs. Losses\")\n",
    "ax1.plot(losses, mse_list_train, label='train')\n",
    "ax1.plot(losses, mse_list_test, label='test')\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90f07e-4a21-4a90-bb8f-ea0a661e0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 'linear'\n",
    "\n",
    "# select the best max depth\n",
    "max_depths = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "rsquared_list_train = []\n",
    "mse_list_train = []\n",
    "rsquared_list_test = []\n",
    "mse_list_test = []\n",
    "for d in max_depths:\n",
    "    regressor = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=d), random_state=13, loss=best_loss, learning_rate=lr_hint)\n",
    "    regressor.fit(train_X, train_y)\n",
    "    rsquared_train, mse_train = calculate_rsquared_and_mse_adaboost(regressor, train_X, train_y)\n",
    "    rsquared_test, mse_test = calculate_rsquared_and_mse_adaboost(regressor, test_X, test_y)\n",
    "    rsquared_list_train.append(rsquared_train)\n",
    "    mse_list_train.append(mse_train)\n",
    "    rsquared_list_test.append(rsquared_test)\n",
    "    mse_list_test.append(mse_test)\n",
    "    \n",
    "# draw a plot of R squared scores vs. max depth and MSE vs. max depth\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "\n",
    "ax0.set_title(\"R-squared vs. Max Depth\")\n",
    "ax0.plot(max_depths, rsquared_list_train, label=\"train\")\n",
    "ax0.plot(max_depths, rsquared_list_test, label='test')\n",
    "for tick in ax0.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "ax1.set_title(\"MSE vs. Max Depth\")\n",
    "ax1.plot(max_depths, mse_list_train, label='train')\n",
    "ax1.plot(max_depths, mse_list_test, label='test')\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(75)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26284bb1-0c8f-457f-97c5-2535d10222a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_depth = 8\n",
    "\n",
    "# since there is a trade-off between learning rate and number of estimators, select a best combination of them\n",
    "learning_rates = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "estimator_nums = [5, 10, 20, 50, 100, 200, 500]\n",
    "for lr in learning_rates:\n",
    "    rsquared_list_train = []\n",
    "    mse_list_train = []\n",
    "    rsquared_list_test = []\n",
    "    mse_list_test = []\n",
    "    for num_est in estimator_nums:\n",
    "        regressor = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=best_max_depth), random_state=13,\n",
    "                                      loss=best_loss, learning_rate=lr, n_estimators=num_est)\n",
    "        regressor.fit(train_X, train_y)\n",
    "        rsquared_train, mse_train = calculate_rsquared_and_mse_adaboost(regressor, train_X, train_y)\n",
    "        rsquared_test, mse_test = calculate_rsquared_and_mse_adaboost(regressor, test_X, test_y)\n",
    "        rsquared_list_train.append(rsquared_train)\n",
    "        mse_list_train.append(mse_train)\n",
    "        rsquared_list_test.append(rsquared_test)\n",
    "        mse_list_test.append(mse_test)\n",
    "    \n",
    "    # convert estimator_nums to strings to be uniformly shown in the plot\n",
    "    estimator_nums_label = [ str(num_est) for num_est in estimator_nums ]\n",
    "    \n",
    "    # draw a plot of R squared scores vs. number of estimators and MSE vs. number of estimators\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(16, 4))\n",
    "    \n",
    "    ax0.set_title(\"R-squared vs. Number of Estimators (lr={})\".format(lr))\n",
    "    ax0.plot(estimator_nums_label, rsquared_list_train, label=\"train\")\n",
    "    ax0.plot(estimator_nums_label, rsquared_list_test, label='test')\n",
    "    for tick in ax0.get_xticklabels():\n",
    "        tick.set_rotation(75)\n",
    "    \n",
    "    ax1.set_title(\"MSE vs. Number of Estimators (lr={})\".format(lr))\n",
    "    ax1.plot(estimator_nums_label, mse_list_train, label='train')\n",
    "    ax1.plot(estimator_nums_label, mse_list_test, label='test')\n",
    "    for tick in ax1.get_xticklabels():\n",
    "        tick.set_rotation(75)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b4787-2d47-46f9-9bd6-c711284a17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = 0.2\n",
    "best_num_estimators = 20\n",
    "\n",
    "# the final AdaBoostRegressor\n",
    "ada_regressor = AdaBoostRegressor(estimator=DecisionTreeRegressor(max_depth=best_max_depth), random_state=13, \n",
    "                                      loss=best_loss, learning_rate=best_lr, n_estimators=best_num_estimators)\n",
    "ada_regressor.fit(train_X, train_y)\n",
    "\n",
    "# check importance of features in AdaBoostRegressor\n",
    "result = permutation_importance(ada_regressor, train_X, train_y, n_repeats=10, random_state=0)\n",
    "print(\"feature importances:\")\n",
    "print(result['importances_mean'])\n",
    "\n",
    "# metrics: R-squared and MSE\n",
    "ada_rsquared_train, ada_mse_train = calculate_rsquared_and_mse_adaboost(ada_regressor, train_X, train_y)\n",
    "ada_rsquared_test, ada_mse_test = calculate_rsquared_and_mse_adaboost(ada_regressor, test_X, test_y)\n",
    "\n",
    "print(\"training R-squared: {}\".format(ada_rsquared_train))\n",
    "print(\"testing R-squared: {}\".format(ada_rsquared_test))\n",
    "print(\"training MSE: {}\".format(ada_mse_train))\n",
    "print(\"testing MSE: {}\".format(ada_mse_test))\n",
    "\n",
    "# plot\n",
    "methods = ['OLS', 'AdaBoostRegressor']\n",
    "fig, (ax0, ax1) = plt.subplots(figsize=(10, 10), nrows=2, ncols=2)\n",
    "\n",
    "ax0[0].set_title(\"R-squared (training)\")\n",
    "ax0[0].bar(methods, [reg_rsquared_train, ada_rsquared_train])\n",
    "\n",
    "ax0[1].set_title(\"R-squared (testing)\")\n",
    "ax0[1].bar(methods, [reg_rsquared_test, ada_rsquared_test])\n",
    "\n",
    "ax1[0].set_title(\"MSE (training)\")\n",
    "ax1[0].bar(methods, [reg_mse_train, ada_mse_train])\n",
    "\n",
    "ax1[1].set_title(\"MSE (testing)\")\n",
    "ax1[1].bar(methods, [reg_mse_test, ada_mse_test])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
